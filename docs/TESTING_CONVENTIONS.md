В документе описаны договорённости по тестированию в проекте HomePlanner. Язык документа — русский. Основной упор сделан на автоматические тесты клиентских приложений и веб-интерфейса.

## 1. Цели и принципы
- Обеспечить воспроизводимое, автоматизируемое и прозрачное тестирование для всех компонентов системы.
- Минимизировать регрессии за счёт раннего обнаружения дефектов.
- Соблюдать единые стандарты написания и запуска тестов независимо от платформы.
- Все автоматические тесты должны быть самодостаточными, изолированными и детерминированными.

## 1.1 ВАЖНО ДЛЯ AI-ИНСТРУМЕНТОВ

**ОБЯЗАТЕЛЬНЫЕ ПРАВИЛА при запуске тестов Python: ДВА КОРРЕКТНЫХ СПОСОБА**

1. **НЕ использовать `cd` в командах** — рабочая директория уже установлена в корень проекта автоматически. Все команды выполняются относительно корня проекта без использования `cd`.
2. **Обязательно активировать `.venv` ПЕРЕД запуском тестов** — это должна быть ОТДЕЛЬНАЯ команда:
   ```bash
   source .venv/bin/activate
   ```
3. **Запуск тестов — ОТДЕЛЬНАЯ команда после активации**. Разрешены ровно два варианта:
   - Вариант 1: `uv run pytest tests/ -v`
   - Вариант 2: `pytest tests/ -v`

**НЕПРАВИЛЬНО (использование `.venv/bin/python -m pytest` без активации):**
```bash
.venv/bin/python -m pytest tests/ -v  # НЕПРАВИЛЬНО!
```

**ПРАВИЛЬНО (отдельная активация + отдельная команда запуска тестов одним из двух способов):**
```bash
source .venv/bin/activate  # Отдельная команда 1
pytest tests/ -v          # Отдельная команда 2 (или: uv run pytest tests/ -v)
```

## 2. Общие требования
- **Рабочая директория**: Все команды выполняются из корня проекта. Рабочая директория уже установлена автоматически (для AI-инструментов, CI/CD, автоматизированных скриптов). **НЕ используйте `cd` в командах** — это относится ко всем способам выполнения.
- Фреймворк для Python-компонентов — `pytest`. Модуль `unittest` не использовать.
- Все тесты размещаются в каталоге `./tests`, сохраняя зеркальную структуру.
- Обязательна аннотация типов и наличие docstring для каждого тестового модуля, класса и функции.
- Тестовые данные и фикстуры должны быть минимальными и описательными. Общие фикстуры выносить в `conftest.py`.
- Корневой `conftest.py` размещается в каталоге `tests/` и содержит фикстуры/фабрики, используемые несколькими модулями; в подпакетах допускаются дополнительные `conftest.py` для доменных сценариев.
- Для мокирования использовать `pytest-mock` (`MockerFixture`) или стандартные средства `pytest`. Глобальные моки ограничивать сроком жизни теста.
- Локальные настройки и секреты не хранить в тестах. Использовать отдельный конфиг файл.
- Общие фабрики данных размещаются в `tests/factories` (или соседних доменных подпапках) и импортируются в тесты через `conftest.py`, чтобы избежать дублирования.
- Повторно используемые хелперы (API-пути, нормализация времени, запуск клиентов) выносить в отдельные библиотеки (`tests/utils.py`, `tests/factories/`) и подключать через импорт вместо дублирования кода.
- Логи тестов должны быть информативными (см. `LogCaptureFixture`), но не зашумлять вывод CI. Дефолтный уровень логирования для локальных и CI-прогонов — `WARNING`; снижение уровня оформляется в тесте явно. Рекомендуемый формат сообщения — `<module>::<scenario>::<context>`.
- **Запуск тестов**: доступны ДВА способа запуска тестов Python (всегда после активации `.venv` отдельной командой):
  
**Способ 1: Через uv (рекомендуется)**
```bash
# Команда 1: Активировать виртуальное окружение (ОТДЕЛЬНАЯ команда)
source .venv/bin/activate  # Linux/Mac
# или
.venv\Scripts\activate     # Windows

# Команда 2: Запустить тесты через uv (ОТДЕЛЬНАЯ команда)
uv run pytest tests/ -v
```
  
**Способ 2: Напрямую через pytest (без uv)**
  
  Команды выполняются **ОТДЕЛЬНО** (активация и запуск — это две разные команды):
  
  ```bash
  # Команда 1: Активировать виртуальное окружение (ОТДЕЛЬНАЯ команда)
  source .venv/bin/activate  # Linux/Mac
  # или
  .venv\Scripts\activate  # Windows
  
  # Команда 2: Запустить тесты (ОТДЕЛЬНАЯ команда после активации)
  pytest tests/ -v
  ```
  
  **ВАЖНО**: 
  - Рабочая директория уже установлена в корень проекта автоматически (для AI-инструментов, CI/CD, автоматизированных скриптов).
  - Все команды выполняются относительно корня проекта без использования `cd`.
  - **НЕ используйте `cd` в командах** — это относится ко всем способам выполнения (вручную, автоматически, через AI-инструменты, в CI/CD).
  - В обоих способах: активация `.venv` и запуск тестов — это **ОТДЕЛЬНЫЕ команды**.
  - После активации используйте либо `uv run pytest`, либо `pytest` напрямую (не `.venv/bin/python -м pytest`).
  
- Последовательность локального запуска python-тестов:
  1. Активировать `.venv` отдельной командой: `source .venv/bin/activate`.
  2. Запустить базовый прогон одним из двух способов:
     - `uv run pytest`
     - или `pytest`
  3. Для покрытия перед публикацией — аналогично:
     - `uv run pytest --cov=backend --cov-report=term-missing`
     - или `pytest --cov=backend --cov-report=term-missing`
  
  **Важно**: При использовании активации `.venv` команды активации и запуска тестов должны быть отдельными.
- Те же команды закреплены в GitHub Actions `.github/workflows/ci-python-tests.yml`, `.github/workflows/ci-frontend-tests.yml` и `.github/workflows/ci-android.yml`. При обновлении workflow необходимо синхронизировать документ.
- Общие правила расположения артефактов и документов описаны в `docs/ARTIFACTS_LAYOUT.md`; при добавлении новых тестовых пакетов ссылаться на соответствующие разделы.

## 2.1 Оптимизация производительности тестов

### Проблема производительности

Основная проблема производительности тестов связана с созданием и удалением схемы базы данных на каждый тест. Операции `Base.metadata.create_all()` и `Base.metadata.drop_all()` выполняются медленно (около 1.5-2 секунд на каждую операцию) и при большом количестве тестов значительно замедляют выполнение всего набора.

### Решение: оптимизация scope фикстур БД

Для оптимизации производительности тестов используется следующий подход:

1. **Фикстура `db_setup` с `scope="module"`** — создает схему БД один раз для всего модуля тестов:
   ```python
   @pytest.fixture(scope="module")
   def db_setup() -> Generator[None, None, None]:
       """Create test database schema once per module."""
       Base.metadata.create_all(bind=engine)
       yield
       Base.metadata.drop_all(bind=engine)
   ```

2. **Фикстура `db_session` с `scope="function"`** — создает новую сессию для каждого теста и очищает данные между тестами:
   ```python
   @pytest.fixture(scope="function")
   def db_session(db_setup: None) -> Generator[Session, None, None]:
       """Create test database session and clean data between tests using optimized DELETE."""
       from sqlalchemy import text
       
       db = SessionLocal()
       try:
           # Clean all tables before each test
           # Disable foreign key checks for faster deletion, then re-enable
           with db.begin():
               db.execute(text("PRAGMA foreign_keys = OFF"))
               # Delete all data - order doesn't matter with FK checks disabled
               db.execute(text("DELETE FROM task_users"))
               db.execute(text("DELETE FROM task_history"))
               db.execute(text("DELETE FROM tasks"))
               db.execute(text("DELETE FROM events"))
               db.execute(text("DELETE FROM groups"))
               db.execute(text("DELETE FROM users"))
               db.execute(text("DELETE FROM app_metadata"))
               db.execute(text("PRAGMA foreign_keys = ON"))
           db.commit()
           yield db
       finally:
           db.rollback()
           db.close()
   ```
   
   **Оптимизация**: Использование `PRAGMA foreign_keys = OFF` перед DELETE ускоряет очистку данных, так как SQLite не проверяет внешние ключи при удалении. Это позволяет удалять данные в любом порядке и быстрее выполнять операции.

### Преимущества оптимизации

- **Быстрее**: БД создается один раз для всего модуля, а не на каждый тест
- **Изоляция**: данные очищаются между тестами через DELETE запросы
- **Стабильность**: тесты остаются изолированными и детерминированными

### Результаты оптимизации

До оптимизации:
- 175 вызовов DDL операций за 308 секунд
- ~1.76 секунды на создание/удаление БД на каждый тест

После оптимизации:
- DDL операции выполняются один раз на модуль
- Экономия времени: ~1.5-2 секунды × количество тестов в модуле

### Применение оптимизации

Все тестовые файлы, использующие БД, должны применять этот паттерн:
- `test_group_service.py`
- `test_task_service.py`
- `test_tasks_router.py`
- `test_tasks.py`
- `test_task_completion_dates.py`
- `test_groups_router.py`
- `test_events.py`
- `test_today_user_filter.py`

### Профилирование тестов

Для анализа производительности тестов используется `pytest-profiling`:

```bash
# Установка
pip install pytest-profiling

# Запуск с профилированием
pytest tests/ -v --profile

# Генерация SVG графиков
pytest tests/ -v --profile-svg

# Сохранение профилей в директорию
pytest tests/ -v --profile --pstats-dir=profiles
```

Профилирование помогает выявить узкие места в тестах и оптимизировать их производительность.

## 3. Автоматические тесты клиентских приложений (Android)
- **Инструментарий**: Kotlin + JUnit5/AndroidX Test для модульных и инструментальных тестов. Для сквозных сценариев — Espresso или UI Automator.
- **Структура**:
  - Модульные тесты Kotlin находятся в `android/app/src/test/java`.
  - Инструментальные тесты — в `android/app/src/androidTest/java`.
- **Соглашения**:
  - Именование классов тестов: `<ClassName>Test.kt` или `<FeatureName>Spec.kt`.
  - Каждый тест описывает ожидаемое поведение (`fun someCondition_expectedResult()`).
  - Использовать `MockK` или встроенные фреймворки для изоляции зависимостей.
  - При работе с базой данных и сетевыми слоями применять фейковые реализации или эмуляторы.
  - Для UI-тестов гарантировать стабильные идентификаторы `View` (resource-id/contentDescription).
- **Запуск**: описать команды в `docs/DEVELOPMENT.md` и поддерживать Gradle-задачи (`./gradlew test`, `./gradlew connectedDebugAndroidTest`). Минимальный smoke-набор приведён в таблице «Smoke-наборы Android».
- **Отчётность**: хранить артефакты (JUnit XML, скриншоты) в `android/app/build/reports`. CI вытягивает их после прогона.

### 3.1 Smoke-наборы Android

| Сценарий | Описание | Тип теста |
| --- | --- | --- |
| `Создание задачи` | Создать задачу с напоминанием и убедиться, что приходит уведомление в установленный слот | Инструментальный |
| `Синхронизация и редактирование` | Активировать синхронизацию, изменить параметры задачи и подтвердить обновление статуса | Интеграционный |
| `Настройки напоминаний` | Переключить напоминания в настройках приложения и проверить сохранение состояния | UI/E2E |

## 4. Автоматические тесты веб-клиента
- **Стек**: `vitest` для модульных/юнит-тестов, `playwright` или `cypress` для end-to-end. Предпочтительно `playwright` для единообразия и интеграции с CI.
- **Структура**:
  - Юнит-тесты — `frontend/tests/unit`. Именование файлов: `<module>.spec.js` или `.ts` (при миграции на TypeScript).
  - Интеграционные/компонентные тесты — `frontend/tests/integration`.
  - E2E-скрипты — `frontend/tests/e2e`.
- **Соглашения**:
  - Использовать тестовые util-функции (`frontend/utils`) для подготовки данных.
  - Не обращаться напрямую к реальному backend; использовать мок-сервер (MSW) или Playwright fixtures.
  - Прежде чем фиксировать селекторы, обеспечивать устойчивость: data-testid, aria-label, роль.
  - Покрывать не только happy-path, но и негативные сценарии (ошибки сети, пустые состояния).
- **Запуск**:
  - Юнит: `npm run test` (или аналогичная команда через `uv` при конвертации).
  - E2E: `npx playwright test` с настройкой CI-профиля (`playwright.config.ts`). Обязательные сценарии сведены в таблицу «Smoke-наборы веб-клиента» и выполняются на каждый merge request.
- **Отчётность**: сохранять отчёты в `frontend/tests/reports`. CI публикует HTML-репорты и прикладывает скриншоты/видео.

## 5. Интеграция с CI/CD

### 4.1 Smoke-наборы веб-клиента

| Сценарий | Описание | Тип теста |
| --- | --- | --- |
| `Авторизация` | Выполнить вход под тестовым пользователем и проверить редирект на список задач | E2E |
| `Создание задачи` | Создать задачу и убедиться, что она отображается в списке с корректным расписанием | E2E |
| `Редактирование расписания` | Изменить расписание задачи и проверить обновление календаря/виджета | Интеграционный |
| `Удаление задачи` | Удалить задачу и проверить корректное отображение пустого состояния | E2E |
- Все тестовые цепочки должны запускаться в CI при каждом push/PR: backend pytest, Android юнит-тесты, веб-юнит и E2E (в nightly или по тегу).
- Невыеявленные флаки должны помечаться и отслеживаться отдельно. Флаки разрешается временно пропускать только по решению владельца проекта.
- Метрики покрытия (coverage) публикуются в job-артефактах: HTML-отчёт помещается в артефакт `coverage-report`, XML (`coverage.xml`) — в `coverage-summary` для интеграции с Codecov. Для python-домена также выгружается `coverage.lcov` в артефакт `coverage-lcov` и загружается в Codecov. Порог покрытия: не ниже 80% для критичных модулей.
- Для тяжёлых E2E допускается nightly-запуск, но smoke-набор (минимум 3 ключевых сценария) должен выполняться на каждый merge request.

## 6. Документация и поддержка
- Любые новые тестовые сценарии сопровождаются записью в `CHANGELOG_HISTORY.md` при значимых изменениях.
- Руководство по локальному запуску тестов обновляется в `docs/DEVELOPMENT.md`.
- Ответственный за тестирование компонент фиксируется в задаче/issue. Важно поддерживать актуальность контактов.
- В случае обнаружения дефектов тестового окружения — фиксировать в баг-трекере с шагами воспроизведения и ссылкой на failing test.

## 7. Порядок внедрения
- Этап 1: согласовать стек инструментов (Playwright/Cypress, MockK и т. п.) и описать команды запуска.
- Этап 2: покрыть smoke-сценарии веб-клиента и критичные бизнес-операции в Android.
- Этап 3: расширять покрытия, внедрять проверки в CI, отслеживать флаки.
- Этап 4: поддерживать регресс-набор к релизам, формировать отчёты и ретроспективы по качеству.

---
Документ подлежит регулярному пересмотру по мере развития продукта и инфраструктуры тестирования.

